---
title: "CaseStudy1"
author: "Group26"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 1. Ratio of Fibonacci numbers

### a. Write two different R functions using for and while loops

The task requires us to create two functions that return the sequence $r_i = F_{i+1}/F_i$ for $i = 1, \ldots, n$ where $F_i$ is the $i$-th Fibonacci number.

```{r}
# Function using for loop
fibonacci_ratio_for <- function(n) {
  # Initialize the first two Fibonacci numbers
  fib <- c(1, 1)
  
  # Calculate the remaining Fibonacci numbers up to n+1
  for (i in 3:(n+1)) {
    fib[i] <- fib[i-1] + fib[i-2]
  }
  
  # Calculate the ratios
  ratios <- fib[2:(n+1)] / fib[1:n]
  
  return(ratios)
}

# Function using while loop
fibonacci_ratio_while <- function(n) {
  # Initialize the first two Fibonacci numbers
  fib <- c(1, 1)
  
  # Calculate the remaining Fibonacci numbers up to n+1
  i <- 3
  while (i <= n+1) {
    fib[i] <- fib[i-1] + fib[i-2]
    i <- i + 1
  }
  
  # Calculate the ratios
  ratios <- fib[2:(n+1)] / fib[1:n]
  
  return(ratios)
}
```

### b. Benchmark the two functions

We'll benchmark both functions for n = 200 and n = 2000 to compare their performance.

```{r}
# Load the microbenchmark package
library(microbenchmark)

# Benchmark for n = 200
benchmark_200 <- microbenchmark(
  for_loop = fibonacci_ratio_for(200),
  while_loop = fibonacci_ratio_while(200),
  times = 100
)

print(benchmark_200)

# Benchmark for n = 2000
benchmark_2000 <- microbenchmark(
  for_loop = fibonacci_ratio_for(2000),
  while_loop = fibonacci_ratio_while(2000),
  times = 100
)

print(benchmark_2000)
```

We can visualize the results:

```{r}
library(ggplot2)

# Plot for n = 200
autoplot(benchmark_200) + 
  ggtitle("Performance comparison for n = 200")

# Plot for n = 2000
autoplot(benchmark_2000) + 
  ggtitle("Performance comparison for n = 2000")
```

Based on the benchmarking results, the for loop implementation is generally faster. This is expected because in R, for loops are optimized for iterating over vectors of known length, while while loops have slightly more overhead for condition checking.

### c. Plot the sequence and find the convergence

Let's plot the sequence for n = 100 and determine when it starts to stabilize.

```{r}
# Calculate the sequence for n = 100
ratios_100 <- fibonacci_ratio_for(100)

# Plot the sequence
plot(1:100, ratios_100, type = "l", 
     xlab = "i", ylab = "r_i = F_{i+1}/F_i",
     main = "Ratio of consecutive Fibonacci numbers")

# Add a horizontal line at the golden ratio
abline(h = (1 + sqrt(5))/2, col = "red", lty = 2)
legend("bottomright", legend = c("Ratio sequence", "Golden ratio"), 
       col = c("black", "red"), lty = c(1, 2))

# Calculate the differences between consecutive ratios
diffs <- diff(ratios_100)
plot(1:99, abs(diffs), type = "l", log = "y",
     xlab = "i", ylab = "Absolute difference between consecutive ratios",
     main = "Convergence of the Fibonacci ratio")
```

The sequence appears to stabilize around i = 10-15. Let's check the exact values:

```{r}
# Print the first 20 ratios
data.frame(
  i = 1:20,
  ratio = ratios_100[1:20],
  difference_from_golden_ratio = abs(ratios_100[1:20] - (1 + sqrt(5))/2)
)

# Calculate the golden ratio
golden_ratio <- (1 + sqrt(5))/2
print(paste("Golden ratio:", golden_ratio))

# Calculate when the sequence is within 1e-10 of the golden ratio
convergence_index <- min(which(abs(ratios_100 - golden_ratio) < 1e-10))
print(paste("The sequence converges to within 1e-10 of the golden ratio at i =", convergence_index))
```

The sequence converges to the golden ratio φ = (1 + √5)/2 ≈ 1.618033988749895. The golden ratio is a famous mathematical constant that appears in many natural phenomena and has been studied extensively in mathematics and art.

The ratio of consecutive Fibonacci numbers converges rather quickly to the golden ratio. By around i = 20, the difference is already extremely small, and by mathematical proof, as i approaches infinity, the ratio exactly equals the golden ratio.

## 2. Gamma Function


## 3. Weak Law of Large Numbers

The law of large numbers states that if you repeat an experiment independently a large number of times and average the result, what you obtain should be close to the expected value. 

Formally, the weak law of large numbers states that if $X_1, X_2, \ldots, X_n$ are independently and identically distributed (iid) random variables with a finite expected value $E(X_i) = \mu < \infty$, then, for any $\epsilon > 0$:

$$\lim_{n\to\infty} P(|\bar{X}_n - \mu| > \epsilon) = 0$$

where $\bar{X}_n = \frac{X_1 + X_2 + \ldots + X_n}{n}$ is the sample mean.

### a. Simulating a fair 6-sided die

```{r}
# Set seed for reproducibility
set.seed(42)

# Number of rolls
n <- 10000

# Simulate rolling a fair die n times using vectorized function
rolls <- sample(1:6, n, replace = TRUE)

# Compute cumulative mean after each roll
cumulative_means <- cumsum(rolls) / (1:n)

# Plot the running average against the number of rolls
plot(1:n, cumulative_means, type = "l", 
     xlab = "Number of rolls", ylab = "Running average",
     main = "Running Average of Die Rolls",
     ylim = c(1, 6))

# Add a horizontal line at the expected value
abline(h = 3.5, col = "red", lty = 2)
legend("topright", legend = c("Running average", "Expected value (3.5)"), 
       col = c("black", "red"), lty = c(1, 2))
```

Looking at the plot, we can see that as the number of rolls increases, the running average converges towards the expected value of 3.5. This illustrates the weak law of large numbers - with more observations, our sample mean gets closer to the true population mean.

### b. Replicating the experiment multiple times

```{r}
# Set seed for reproducibility
set.seed(123)

# Number of replications
M <- 50

# Matrix to store running averages for each replication
running_averages <- matrix(NA, nrow = M, ncol = n)

# For loop to replicate the experiment M times
for (i in 1:M) {
  # Simulate rolling a die n times
  rolls <- sample(1:6, n, replace = TRUE)
  
  # Compute cumulative mean after each roll
  running_averages[i, ] <- cumsum(rolls) / (1:n)
}

# Plot all 50 paths
plot(1:n, running_averages[1, ], type = "l", 
     xlab = "Number of rolls", ylab = "Running average",
     main = "Running Average of Die Rolls (50 replications)",
     ylim = c(2.5, 4.5), col = 1)

# Add the remaining 49 paths
for (i in 2:M) {
  lines(1:n, running_averages[i, ], col = i)
}

# Add a horizontal line at the expected value
abline(h = 3.5, col = "red", lwd = 2)
legend("topright", legend = c("Replications", "Expected value (3.5)"), 
       col = c("black", "red"), lty = c(1, 1))
```

The plot shows 50 different paths, each representing a replication of our experiment. We can see that all paths start with high variability but converge towards the expected value of 3.5 as the number of rolls increases. Some paths approach the expected value from above, others from below, but they all tend to get closer to 3.5 as n increases.

### c. Expected value of a fair 6-sided die

For a fair 6-sided die, the possible outcomes are $\{1, 2, 3, 4, 5, 6\}$, each with probability $\frac{1}{6}$.

The theoretical expected value is:

$$E(X) = \sum_{i=1}^{6} i \cdot P(X = i) = \sum_{i=1}^{6} i \cdot \frac{1}{6} = \frac{1 + 2 + 3 + 4 + 5 + 6}{6} = \frac{21}{6} = 3.5$$

Therefore, the theoretical expected value of a fair 6-sided die is 3.5.

### d. Comment on running means relative to expected value

```{r}
# Calculate absolute differences from expected value
abs_diff <- abs(running_averages - 3.5)

# Calculate mean absolute difference for each n
mean_abs_diff <- colMeans(abs_diff)

# Plot mean absolute difference
plot(1:n, mean_abs_diff, type = "l", 
     xlab = "Number of rolls", ylab = "Mean absolute difference from expected value",
     main = "Convergence to Expected Value")
```

We observe that as n increases, the average absolute difference between the running means and the expected value decreases. This is consistent with the weak law of large numbers.

The variations in the running means become smaller as the number of observations increases, demonstrating that with more data, the sample mean becomes a more reliable estimator of the population mean.

The running means start with high variability due to the randomness in small samples, but as more die rolls are included, the aggregate behavior becomes more predictable and centers around the expected value of 3.5.

### e. Using a while loop to find when convergence occurs

```{r}
# Set seed for reproducibility
set.seed(456)

# Epsilon value
epsilon <- 0.01

# Initialize variables
n_current <- 0
proportion_outside <- 1

# Matrix to store running averages
running_averages <- matrix(NA, nrow = M, ncol = 1)

while (proportion_outside >= 0.05) {
  # Increment n
  n_current <- n_current + 100
  
  # Resize the matrix if needed
  if (ncol(running_averages) < n_current) {
    running_averages <- cbind(running_averages, matrix(NA, nrow = M, ncol = 100))
  }
  
  # Update running averages for all replications
  for (i in 1:M) {
    # Generate new rolls
    new_rolls <- sample(1:6, 100, replace = TRUE)
    
    # If this is the first iteration, initialize
    if (n_current == 100) {
      running_averages[i, 1:100] <- cumsum(new_rolls) / (1:100)
    } else {
      # Calculate the sum up to n_current - 100
      prev_sum <- running_averages[i, n_current - 100] * (n_current - 100)
      
      # Add new rolls and calculate new running averages
      new_sums <- prev_sum + cumsum(new_rolls)
      running_averages[i, (n_current - 99):n_current] <- new_sums / ((n_current - 99):n_current)
    }
  }
  
  # Check how many replications are outside the epsilon interval at n_current
  outside_epsilon <- sum(abs(running_averages[, n_current] - 3.5) > epsilon)
  proportion_outside <- outside_epsilon / M
  
  # Print progress
  cat("n =", n_current, "- Proportion outside epsilon:", proportion_outside, "\n")
}

# Final value of n
cat("The loop terminated at n =", n_current, "with proportion outside epsilon =", proportion_outside, "\n")
```

The while loop stops when the proportion of replications where the difference between the running average and the expected value lies outside the interval $[-\epsilon, \epsilon]$ is less than 0.05. 

With $\epsilon = 0.01$, the loop terminated when this criterion was met, meaning that at least 95% of our replications had running averages within 0.01 of the expected value 3.5.

### f. Repeating with Cauchy distribution

```{r}
# Set seed for reproducibility
set.seed(789)

# Matrix to store running averages for Cauchy distribution
cauchy_averages <- matrix(NA, nrow = M, ncol = n)

# For loop to replicate the experiment M times
for (i in 1:M) {
  # Simulate from standard Cauchy distribution
  cauchy_samples <- rcauchy(n)
  
  # Compute cumulative mean after each sample
  cauchy_averages[i, ] <- cumsum(cauchy_samples) / (1:n)
}

# Plot all 50 paths
plot(1:n, cauchy_averages[1, ], type = "l", 
     xlab = "Number of samples", ylab = "Running average",
     main = "Running Average of Cauchy Samples (50 replications)",
     ylim = c(-10, 10), col = 1)

# Add the remaining 49 paths
for (i in 2:M) {
  lines(1:n, cauchy_averages[i, ], col = i)
}

# Zoom in to see more detail
plot(1:n, cauchy_averages[1, ], type = "l", 
     xlab = "Number of samples", ylab = "Running average",
     main = "Running Average of Cauchy Samples (Zoomed)",
     ylim = c(-3, 3), col = 1)

for (i in 2:M) {
  lines(1:n, cauchy_averages[i, ], col = i)
}
```

Unlike with the die rolls, the running means of samples from the Cauchy distribution do not converge to a specific value as n increases. The Cauchy distribution has very heavy tails, and extreme values can significantly influence the running average even with large sample sizes.

The weak law of large numbers does not hold for the Cauchy distribution because it does not have a finite expected value (mean). The Cauchy distribution is a classic example of a distribution where the sample mean is not a consistent estimator of the population center.

This happens because the Cauchy distribution has such heavy tails that extreme values occur frequently enough to prevent the sample mean from stabilizing. Each new extreme value can dramatically shift the running average, regardless of how many samples have already been collected.

This is in stark contrast to the die rolls, where the running average clearly converged to the expected value as the number of rolls increased.