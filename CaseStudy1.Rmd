---
title: "CaseStudy1"
author: "Group26"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 1. Ratio of Fibonacci numbers

### a. Write two different R functions using for and while loops

The task requires us to create two functions that return the sequence $r_i = F_{i+1}/F_i$ for $i = 1, \ldots, n$ where $F_i$ is the $i$-th Fibonacci number.

```{r}
# Function using for loop
fibonacci_ratio_for <- function(n) {
  # Initialize the first two Fibonacci numbers
  fib <- c(1, 1)
  
  # Calculate the remaining Fibonacci numbers up to n+1
  for (i in 3:(n+1)) {
    fib[i] <- fib[i-1] + fib[i-2]
  }
  
  # Calculate the ratios
  ratios <- fib[2:(n+1)] / fib[1:n]
  
  return(ratios)
}

# Function using while loop
fibonacci_ratio_while <- function(n) {
  # Initialize the first two Fibonacci numbers
  fib <- c(1, 1)
  
  # Calculate the remaining Fibonacci numbers up to n+1
  i <- 3
  while (i <= n+1) {
    fib[i] <- fib[i-1] + fib[i-2]
    i <- i + 1
  }
  
  # Calculate the ratios
  ratios <- fib[2:(n+1)] / fib[1:n]
  
  return(ratios)
}
```

### b. Benchmark the two functions

We'll benchmark both functions for n = 200 and n = 2000 to compare their performance.

```{r}
# Load the microbenchmark package
library(microbenchmark)

# Benchmark for n = 200
benchmark_200 <- microbenchmark(
  for_loop = fibonacci_ratio_for(200),
  while_loop = fibonacci_ratio_while(200),
  times = 100
)

print(benchmark_200)

# Benchmark for n = 2000
benchmark_2000 <- microbenchmark(
  for_loop = fibonacci_ratio_for(2000),
  while_loop = fibonacci_ratio_while(2000),
  times = 100
)

print(benchmark_2000)
```

We can visualize the results:

```{r}
library(ggplot2)

# Plot for n = 200
autoplot(benchmark_200) + 
  ggtitle("Performance comparison for n = 200")

# Plot for n = 2000
autoplot(benchmark_2000) + 
  ggtitle("Performance comparison for n = 2000")
```

Based on the benchmarking results, the for loop implementation is generally faster. This is expected because in R, for loops are optimized for iterating over vectors of known length, while while loops have slightly more overhead for condition checking.

### c. Plot the sequence and find the convergence

Let's plot the sequence for n = 100 and determine when it starts to stabilize.

```{r}
# Calculate the sequence for n = 100
ratios_100 <- fibonacci_ratio_for(100)

# Plot the sequence
plot(1:100, ratios_100, type = "l", 
     xlab = "i", ylab = "r_i = F_{i+1}/F_i",
     main = "Ratio of consecutive Fibonacci numbers")

# Add a horizontal line at the golden ratio
abline(h = (1 + sqrt(5))/2, col = "red", lty = 2)
legend("bottomright", legend = c("Ratio sequence", "Golden ratio"), 
       col = c("black", "red"), lty = c(1, 2))

# Calculate the differences between consecutive ratios
diffs <- diff(ratios_100)
plot(1:99, abs(diffs), type = "l", log = "y",
     xlab = "i", ylab = "Absolute difference between consecutive ratios",
     main = "Convergence of the Fibonacci ratio")
```

The sequence appears to stabilize around i = 10-15. Let's check the exact values:

```{r}
# Print the first 20 ratios
data.frame(
  i = 1:20,
  ratio = ratios_100[1:20],
  difference_from_golden_ratio = abs(ratios_100[1:20] - (1 + sqrt(5))/2)
)

# Calculate the golden ratio
golden_ratio <- (1 + sqrt(5))/2
print(paste("Golden ratio:", golden_ratio))

# Calculate when the sequence is within 1e-10 of the golden ratio
convergence_index <- min(which(abs(ratios_100 - golden_ratio) < 1e-10))
print(paste("The sequence converges to within 1e-10 of the golden ratio at i =", convergence_index))
```

The sequence converges to the golden ratio φ = (1 + √5)/2 ≈ 1.618033988749895. The golden ratio is a famous mathematical constant that appears in many natural phenomena and has been studied extensively in mathematics and art.

The ratio of consecutive Fibonacci numbers converges rather quickly to the golden ratio. By around i = 20, the difference is already extremely small, and by mathematical proof, as i approaches infinity, the ratio exactly equals the golden ratio.

## 2. Gamma Function
In this exercise, we will write a function to compute the following for *n* a positive integer using the gamma  function:
(FORMEL EINFÜGEN)
$a)$

```{r formula, echo=TRUE}
n_calculation <- function(n){
 result <- (gamma((n - 1)/2))/(gamma(1/2) * gamma((n-2)/2))
  print(result)
}

n_calculation(20)
```
$b)$
Now, we will execute the function with $n = 2000$:
```{r 2000 execution, echo=TRUE}

n_calculation(2000)
```
We can observe that we receive a "NaN" as result. It could be, because we calculated the function with a pretty high value (2000). Trying to execute the function with $n = 20$ or $n = 200$ works normally and produces an appropriate result. If we split up the denominator and the nominator and calculate them individually, we receive the following:

```{r formula individual, echo=TRUE}
n_calculation_numerator <- function(n){
 result <- (gamma((n - 1)/2))
  print(result)
}

n_calculation_denominator <- function(n){
 result <- (gamma(1/2) * gamma((n-2)/2))
  print(result)
}
cat("Numerator result for n = 200: \n")
n_calculation_numerator(200)
cat("Denominator result for n = 200: \n")
n_calculation_denominator(200)

cat("Numerator result for n = 2000: \n")
n_calculation_numerator(2000)
cat("Denominator result for n = 2000: \n")
n_calculation_denominator(2000)
```
We can observe that already the result for $n = 200$ is very high. Both results for numerator and denominator are even higher and we receive "Infinite" as output there. R is struggling to divide a infinite result with another infinite result und that is why it is returning a "NaN".
$c)$
Here, we will write an implementation that can handle values that are higher than $n = 1000$. As the reason for the unexpected result for $n = 2000$ is the fact that the calculation produces very high numbers, we can shrink them by applying a logarithm on the calculation of all three gamma functions. For that, we will simply use the function *lgamma()*, which calculates the natural logarithm of the gamma function, avoiding the appearence of very high numbers in the calculation (source: https://stackoverflow.com/questions/42821156/how-to-overcome-the-gamma-function-limitation-in-r). At the end, we will need to exponentiate the result again to receive the expected normal values. As we have the logarithmic calculcations in both numerator and denominator, we need to substract or add the result from the denominator from the numerator.

```{r log calculation, echo=TRUE}
n_calculation_logarithmic <- function(n){
 result_numerator <- (lgamma((n - 1)/2))
 result_denominator <- (lgamma(1/2) + lgamma((n-2)/2))
 final_result <- exp(result_numerator - result_denominator)
  print(final_result) 
}

cat("Numerator result for n = 200: \n")
n_calculation_logarithmic(200)


cat("Numerator result for n = 2000: \n")
n_calculation_logarithmic(2000)
```
Printing out the result shows us that the adjusted implementation of the calculation is working well also for high values of n.
$d)$
Now, we will plot the result of $\rho_n / \sqrt{n}$ for different values of n. We will use the present function of $c)$.

```{r plotting for different n, echo=TRUE}
n_calculation_logarithmic <- function(n){
 result_numerator <- (lgamma((n - 1)/2))
 result_denominator <- (lgamma(1/2) + lgamma((n-2)/2))
 result_gamma <- exp(result_numerator - result_denominator)
 result_sqrt <- sqrt(n)
 return(result_gamma / result_sqrt)
}
 
n_values_5000 <- seq(3, 5000, by = 10)
ratio_values_5000 <- sapply(n_values_5000, n_calculation_logarithmic)

plot(n_values_5000, ratio_values_5000, 
     type = "l", 
     xlab = "n", 
     ylab = expression(rho[n] / sqrt(n)),
     main = "5000 values")

n_values_100 <- seq(3, 100, by = 10)
ratio_values_100 <- sapply(n_values_100, n_calculation_logarithmic)

plot(n_values_100, ratio_values_100, 
     type = "l", 
     xlab = "n", 
     ylab = expression(rho[n] / sqrt(n)),
     main = "100 values")
```
At the beginning, we have plotted 5000 different n values and received a function as result that is going towards ~0,4 on the y-aaxis. Plotting the same function with only 100 different values for n, we received a function that is going slower towards 0,4 on the y-axis than the function from the 5000-amount sample. Looking o especially the plot with 5000 different numbers of n, I would guess that the limit for the function for $n \to \infty$ is 0,4, as for larger values the function converges towards that value.

## 3. Weak Law of Large Numbers

The law of large numbers states that if you repeat an experiment independently a large number of times and average the result, what you obtain should be close to the expected value.

Formally, the weak law of large numbers states that if $X_1, X_2, \ldots, X_n$ are independently and identically distributed (iid) random variables with a finite expected value $E(X_i) = \mu < \infty$, then, for any $\epsilon > 0$:

$$\lim_{n\to\infty} P(|\bar{X}_n - \mu| > \epsilon) = 0$$

where $\bar{X}_n = \frac{X_1 + X_2 + \ldots + X_n}{n}$ is the sample mean.

### a. Simulating a fair 6-sided die

```{r}
# Set seed for reproducibility
set.seed(42)

# Number of rolls
n <- 10000

# Simulate rolling a fair die n times using vectorized function
rolls <- sample(1:6, n, replace = TRUE)

# Compute cumulative mean after each roll
cumulative_means <- cumsum(rolls) / (1:n)

# Plot the running average against the number of rolls
plot(1:n, cumulative_means, type = "l", 
     xlab = "Number of rolls", ylab = "Running average",
     main = "Running Average of Die Rolls",
     ylim = c(1, 6))

# Add a horizontal line at the expected value
abline(h = 3.5, col = "red", lty = 2)
legend("topright", legend = c("Running average", "Expected value (3.5)"), 
       col = c("black", "red"), lty = c(1, 2))
```

Looking at the plot, we can see that as the number of rolls increases, the running average converges towards the expected value of 3.5. This illustrates the weak law of large numbers - with more observations, our sample mean gets closer to the true population mean.

### b. Replicating the experiment multiple times

```{r}
# Set seed for reproducibility
set.seed(123)

# Number of replications
M <- 50

# Matrix to store running averages for each replication
running_averages <- matrix(NA, nrow = M, ncol = n)

# For loop to replicate the experiment M times
for (i in 1:M) {
  # Simulate rolling a die n times
  rolls <- sample(1:6, n, replace = TRUE)
  
  # Compute cumulative mean after each roll
  running_averages[i, ] <- cumsum(rolls) / (1:n)
}

# Plot all 50 paths
plot(1:n, running_averages[1, ], type = "l", 
     xlab = "Number of rolls", ylab = "Running average",
     main = "Running Average of Die Rolls (50 replications)",
     ylim = c(2.5, 4.5), col = 1)

# Add the remaining 49 paths
for (i in 2:M) {
  lines(1:n, running_averages[i, ], col = i)
}

# Add a horizontal line at the expected value
abline(h = 3.5, col = "red", lwd = 2)
legend("topright", legend = c("Replications", "Expected value (3.5)"), 
       col = c("black", "red"), lty = c(1, 1))
```

The plot shows 50 different paths, each representing a replication of our experiment. We can see that all paths start with high variability but converge towards the expected value of 3.5 as the number of rolls increases. Some paths approach the expected value from above, others from below, but they all tend to get closer to 3.5 as n increases.

### c. Expected value of a fair 6-sided die

For a fair 6-sided die, the possible outcomes are $\{1, 2, 3, 4, 5, 6\}$, each with probability $\frac{1}{6}$.

The theoretical expected value is:

$$E(X) = \sum_{i=1}^{6} i \cdot P(X = i) = \sum_{i=1}^{6} i \cdot \frac{1}{6} = \frac{1 + 2 + 3 + 4 + 5 + 6}{6} = \frac{21}{6} = 3.5$$

Therefore, the theoretical expected value of a fair 6-sided die is 3.5.

### d. Comment on running means relative to expected value

```{r}
# Calculate absolute differences from expected value
abs_diff <- abs(running_averages - 3.5)

# Calculate mean absolute difference for each n
mean_abs_diff <- colMeans(abs_diff)

# Plot mean absolute difference
plot(1:n, mean_abs_diff, type = "l", 
     xlab = "Number of rolls", ylab = "Mean absolute difference from expected value",
     main = "Convergence to Expected Value")
```

We observe that as n increases, the average absolute difference between the running means and the expected value decreases. This is consistent with the weak law of large numbers.

The variations in the running means become smaller as the number of observations increases, demonstrating that with more data, the sample mean becomes a more reliable estimator of the population mean.

The running means start with high variability due to the randomness in small samples, but as more die rolls are included, the aggregate behavior becomes more predictable and centers around the expected value of 3.5.

### e. Using a while loop to find when convergence occurs

```{r}
# Set seed for reproducibility
set.seed(456)

# Epsilon value
epsilon <- 0.01

# Initialize variables
n_current <- 0
proportion_outside <- 1

# Matrix to store running averages
running_averages <- matrix(NA, nrow = M, ncol = 1)

while (proportion_outside >= 0.05) {
  # Increment n
  n_current <- n_current + 100
  
  # Resize the matrix if needed
  if (ncol(running_averages) < n_current) {
    running_averages <- cbind(running_averages, matrix(NA, nrow = M, ncol = 100))
  }
  
  # Update running averages for all replications
  for (i in 1:M) {
    # Generate new rolls
    new_rolls <- sample(1:6, 100, replace = TRUE)
    
    # If this is the first iteration, initialize
    if (n_current == 100) {
      running_averages[i, 1:100] <- cumsum(new_rolls) / (1:100)
    } else {
      # Calculate the sum up to n_current - 100
      prev_sum <- running_averages[i, n_current - 100] * (n_current - 100)
      
      # Add new rolls and calculate new running averages
      new_sums <- prev_sum + cumsum(new_rolls)
      running_averages[i, (n_current - 99):n_current] <- new_sums / ((n_current - 99):n_current)
    }
  }
  
  # Check how many replications are outside the epsilon interval at n_current
  outside_epsilon <- sum(abs(running_averages[, n_current] - 3.5) > epsilon)
  proportion_outside <- outside_epsilon / M
  
  # Print progress
  cat("n =", n_current, "- Proportion outside epsilon:", proportion_outside, "\n")
}

# Final value of n
cat("The loop terminated at n =", n_current, "with proportion outside epsilon =", proportion_outside, "\n")
```

The while loop stops when the proportion of replications where the difference between the running average and the expected value lies outside the interval $[-\epsilon, \epsilon]$ is less than 0.05.

With $\epsilon = 0.01$, the loop terminated when this criterion was met, meaning that at least 95% of our replications had running averages within 0.01 of the expected value 3.5.

### f. Repeating with Cauchy distribution

```{r}
# Set seed for reproducibility
set.seed(789)

# Matrix to store running averages for Cauchy distribution
cauchy_averages <- matrix(NA, nrow = M, ncol = n)

# For loop to replicate the experiment M times
for (i in 1:M) {
  # Simulate from standard Cauchy distribution
  cauchy_samples <- rcauchy(n)
  
  # Compute cumulative mean after each sample
  cauchy_averages[i, ] <- cumsum(cauchy_samples) / (1:n)
}

# Plot all 50 paths
plot(1:n, cauchy_averages[1, ], type = "l", 
     xlab = "Number of samples", ylab = "Running average",
     main = "Running Average of Cauchy Samples (50 replications)",
     ylim = c(-10, 10), col = 1)

# Add the remaining 49 paths
for (i in 2:M) {
  lines(1:n, cauchy_averages[i, ], col = i)
}

# Zoom in to see more detail
plot(1:n, cauchy_averages[1, ], type = "l", 
     xlab = "Number of samples", ylab = "Running average",
     main = "Running Average of Cauchy Samples (Zoomed)",
     ylim = c(-3, 3), col = 1)

for (i in 2:M) {
  lines(1:n, cauchy_averages[i, ], col = i)
}
```

Unlike with the die rolls, the running means of samples from the Cauchy distribution do not converge to a specific value as n increases. The Cauchy distribution has very heavy tails, and extreme values can significantly influence the running average even with large sample sizes.

The weak law of large numbers does not hold for the Cauchy distribution because it does not have a finite expected value (mean). The Cauchy distribution is a classic example of a distribution where the sample mean is not a consistent estimator of the population center.

This happens because the Cauchy distribution has such heavy tails that extreme values occur frequently enough to prevent the sample mean from stabilizing. Each new extreme value can dramatically shift the running average, regardless of how many samples have already been collected.

This is in stark contrast to the die rolls, where the running average clearly converged to the expected value as the number of rolls increased.

## 4. Central limit theorem

$a)$
First, we will derive the expected value of the sample mean $\bar{X}_n$:

If we put $\bar{X}_n$ inside the expectation operator $E()$, we can apply linearity. Since $\bar{X}_n = \frac{X_1 + X_2 + \cdots + X_n}{n}$, we can put it inside the expectation operator receiving $E(\bar{X}_n) = E\left(\frac{X_1 + X_2 + \cdots + X_n}{n}\right)$. Applying linearity, we can reshape the expectation operator, receiving 

$$E\left(\bar{X}_n\right) = \frac{1}{n}\Big[E(X_1) + E(X_2) + \cdots + E(X_n)\Big]$$
Since all $X_i$ are identically and idenpendently distributed, all of them have the same mean, resulting in $E(X_1) = E(X_2) = E(X_n) = \mu$. With that, we can use the mean in the calculation of the expected operator: $E(\bar{X_n}) = \frac{1}{n}(\mu + \mu + \cdots + \mu) = \frac{n \mu}{n} = \mu$
Thus, $E(\bar{X_n}) = \mu$. So, no matter how many samples we have, the average of the samples will be the mean $\mu$.

$b)$
Second, we will derive the variance of the same sample mean. We can assume the following, as the sample mean is given:

$Var(\bar{X}_n) = Var(\frac{X_1 + X_2 + \cdots + X_n}{n})$

If we have independent random variables, the variance of their sums is the sum of their variances. So, we can write:

$$Var(X_1 + X_2 + \cdots + X_n) = Var(X_1) + Var(X_2) + \cdots + Var(X_n)$$

Further, a constant in the variance operator can be brought out and take position before the variance operator with a multiplication. In our case, we can bring $n$ out of the variance operators. It also needs to be squared, as it is brought out of the variance operator. In the same step, we use the independence of the random variables and reshaping the variance of the sum of random variables in the sum of their variances:

$$Var(\bar{X}_n) = \frac{1}{n^2} [Var(X_1) + Var(X_2) + \cdots + Var(X_n)]$$

Since all random variables are independent and identically distributed, they all have the same variance and we receive:

$$Var(\bar{X}_n\right) = \frac{1}{n^2} (\sigma^2 + \sigma^2 + \cdots + \sigma^2) = \frac{n\sigma^2}{n^2} = \frac{\sigma^2}{n}$$

So, as result we get:

$$Var(\bar{X}_n) = \frac{\sigma^2}{n}$$
 $c)$
 Here, we will consider the die-rolling experiment and simulate die rolls. Further, we will repeat the die rolls $M = 100$ times to receive 100 sample means for each n.
 Afterwards, we will plot a histogram of the 100 sample means of each value of n. We will write a function to simulate the die rolls.
 
```{r die rolling experiment, echo=TRUE}
simulate_die_rolls <- function(n) {
  die_rolls <- sample(1:6, size = n, replace = TRUE)
  mean(die_rolls)
}


cat("Mean of 100 rolls:", simulate_die_rolls(100),"\n")
cat("Mean of 1000 rolls:", simulate_die_rolls(1000),"\n")
cat("Mean of 10000 rolls:", simulate_die_rolls(10000),"\n")
```
Now, we will repeat the process 100 times for each value of n.
```{r die rolling repetition, echo=TRUE}
M <- 100
sample_sizes <- c(100, 1000, 10000)
all_means <- list()

for (n in sample_sizes) {
  means <- numeric(M)
  
  for (i in 1:M) {
    means[i] <- simulate_die_rolls(n)
  }
  
  all_means[[as.character(n)]] <- means
}
print("First 5 means for n=100:")
print(head(all_means[["100"]], 5))
print("First 5 means for n=1000:")
print(head(all_means[["1000"]], 5))
print("First 5 means for n=10000:")
print(head(all_means[["10000"]], 5))
```
Now, we will visualize the distribution for each value of n.
```{r die rolling experiment, echo=TRUE}
for (n in sample_sizes) {
  
  hist(all_means[[as.character(n)]],
       main = paste("n =", n),
       xlab = "Sample Mean",
       xlim = c(3, 4),  
       breaks = 20,
       freq = FALSE)
}
```

We can observe that the variance is the highest with $n = 100$ and lowest with $n = 10000$. If we calculate the mean $(1+2+3+4+5+6)/6 = 3.5$, we can see that the higher the value for $n$, the less the values spread among the real mean.
For the value of $n = 100$, the histogram shows a little bell-shaped form, still containing some abnormal means. With $n = 1000$, the distribution becomes smoother, still having some irregular means. And with $n = 10000$, the distribution of the means closely matches to a normal bell-shaped curve.
According to the *Central Limit Theorem*, the distribution of the means becomes more and more bell-shpaed the more samples we have. Also, the variability of the values decreases to the real mean the higher the value for n is. Also, the higher the value for n is, the more means appear near to the normal mean ($\mu = 3.5$).
Thus, the central limit theorem can be explained by that example, as we could observe that the distribution of our samples becomes normal when we have an especially high value for *n*, even if the distribution of samples for a small value of n is not normal.

$d)$
Here, we will compute the expected value $\mu$ and $\sigma^2$ for the random variable $X_i$, which follows a discrete uniform distribution. 

Following the formula to calculate the mean 
$$\mu = E[X_i] = \sum_{k=1}^{6} k \cdot \frac{1}{6} = \frac{1 + 2 + 3 + 4 + 5 + 6}{6} = \frac{21}{6} = 3.5$$
we can calculate the mean:

```{r mean calculation, echo=TRUE}
mu <- (1 + 2 + 3 + 4 + 5 + 6) / 6
cat("Calculated mean: ", mu, "\n")
```

According the formula to calculate the variance
$$E[X_i^2] = \sum_{k=1}^{6} k^2 \cdot P(X_i = k) = \frac{1^2 + 2^2 + 3^2 + 4^2 + 5^2 + 6^2}{6} = \frac{91}{6}$$

$$\sigma^2 = \frac{91}{6} - 3.5^2 = \frac{35}{12} \approx 2.9167$$

we calculate the variance

```{r variance calculation, echo=TRUE}
variance <- ((1^2 + 2^2 + 3^2 + 4^2 + 5^2 + 6^2)/6) - mu^2
cat("Calculated variance: ", variance, "\n")
```
$e)$
Now, we will again simulate the rolling of n dices and calculate the $Z_n$.
```{r simulation of roll dice, echo=TRUE}
mu <- 3.5
sigma_sq <- 35/12
simulate_zn <- function(n, M) {
  zn_values <- numeric(M)
  for (i in 1:M) {
    sample <- sample(1:6, size = n, replace = TRUE)
    sample_mean <- mean(sample)
    zn <- (sample_mean - mu) / sqrt(sigma_sq/n)
    zn_values[i] <- zn
  }
  return(zn_values)
}

sample_sizes <- c(100, 1000, 10000)
replications <- c(100, 1000, 10000)

for (n in sample_sizes) {
  for (M in replications) {
    zn <- simulate_zn(n, M)
    hist(zn, 
         main = paste("n =", n, ", M =", M),
         xlab = expression(Z[n]),
         freq = FALSE)
  }
}
```
According to the *Central Limit Theory*, the distribution of $Z_n$ should converge to a standard normal distribution. Looking at the histograms that we have created, the assumption can be confirmed. Having $M = 100$, all three histograms show a slightly bell-shaped shape, but still with some outliers according to all values for *n*. This could occur, because the amount of data is much smaller than for higher values of *M*. The higher the value for *M* gets, the more normal the histograms look like (in relation to a normal bell shape), while it has the best bell shape for every value of *n* with $M = 10000$.
A higher value for *n* brings us a histogram with fewer outliers towards the boundaries of the histogram. Also, the smaller the value for *n*, the wider tailed is the distribution of the values. For $n = 10000$, we can see the best fit of the resulting shape with the standard normal distribution.
Thus, we can say that for larger values of *n*, the distribution of $Z_n$ becomes normal, regardless of the original shape. Further, larger values for *M* show this convergence better, because it is possible to compare different values for *M*.


## 5. Readable and Efficient Code

### a. Explanation

**(i)** In the first provided chunk of code, we start by initializing two variables x and z with 100 values of a normal distribution and then, although uncorrelated, fit a linear model of x on z. We then apply the sinus function to the residuals of the model, update x with it and refit the model. This step is repeated 4 times. To prevent x from holding only small values, a threshold is introduced and checked in each iteration. In the end, the transformed x vector is returned.

**(ii)** In the second provided code chunk, again we initialize two variables with a sample from a normal distribution. This time, however they are correlated. The code then evaluates the linear model of y on x in a 4-fold Cross validation manner. First we fit the model on all indices but 0-250, then we make predictions at these indices and calculate the RMSE. We repeat this three more times until all 1000 indices have been part of the test set once. We return the vector r, which holds the 4 RMSE values.

### b. Ideas for Improvement

-   In both code chunks, large parts are repitive and could be replaced by a loop. In particular we could use a loop for the residual computation and fitting process in (i) and for the train-predict-evaluate procedure in (ii).
-   In the loops we could replace hard coded values with variables with descriptive names, e.g. "threshold". Then we can provide variable descriptions at the initialization of these variables and explanations where they are updated.
-   We should use comments to show the purpose of each step and the expected outcome.

### c. Refactored Code

**(i)**

```{r}
x_transformation <- function(){
  #seed for reproducibility
  set.seed(1)  
  
  #initialize x and z from normal distribution (100 values)
  x <- rnorm(100)      
  z <- rnorm(100)      
  
  #4 iterations of transformation
  for(i in 1:4){      
    
    #prevent diminishing values of x
    threshold <- i * 0.001
    if(sum(x >= threshold) < i) stop(paste("Step", i, "requires", i, "obs >= ", thr))
    
    #fit linear model of x on z
    fit <- lm(x ~ z)
    
    #update x with sin(residuals), scale and add offset
    x <- i * sin(fit$residuals) + 0.01 * i  
  }
  #return transformed x
  return(x)
}

x_transformation()
```

**(ii)**

```{r}
lm_cv_4fold <- function(){
  # seed for reproducibility
  set.seed(1)
  
  #initialize x and y from normal distribution (1000 values)
  x <- rnorm(1000)
  y <- 2 + x + rnorm(1000)
  df <- data.frame(x, y)
  r <- c()
  
  for(i in 1:4){
    #fit linear model of y on x on i-th quarter
    test <- ((i-1)*250+1):(i*250) # this is the 11-th line but it makes the code much more readable!
    fit <- lm(y ~ x, data = df[-test,])
    
    #predict y at test_indices
    p <- predict(fit, newdata = df[test,])
    
    #calculate RMSE and append
    r <- c(r, sqrt(mean((df$y[test] - p)^2)))
  }
  return(r)
}
lm_cv_4fold()
```

## 6. Measuring and Improving Performance


```{r}
kwtest <- function(x, g, ...) {
  if (is.list(x)) {
    if (length(x) < 2L)
      stop("'x' must be a list with at least 2 elements")
    if (!missing(g))
    warning("'x' is a list, so ignoring argument 'g'")
    if (!all(sapply(x, is.numeric)))
    warning("some elements of 'x' are not numeric and will be coerced to numeric")
    k <- length(x)
    l <- lengths(x)
    if (any(l == 0L))
    stop("all groups must contain data")
    g <- factor(rep.int(seq_len(k), l))
    x <- unlist(x)
  }
  else{
    if (length(x) != length(g))
    stop("'x' and 'g' must have the same length")
    g <- factor(g)
    k <- nlevels(g)
    if (k < 2L)
    stop("all observations are in the same group")
  }
  
  n <- length(x)
  if (n < 2L)
    stop("not enough observations")
  
  r <- rank(x)
  TIES <- table(x)
  
  STATISTIC <- sum(tapply(r, g, sum)^2/tapply(r, g, length))
  STATISTIC <- ((12 * STATISTIC/(n * (n + 1)) - 3 * (n + 1))/(1 -
  sum(TIES^3 - TIES)/(n^3 - n)))
  
  PARAMETER <- k - 1L
  PVAL <- pchisq(STATISTIC, PARAMETER, lower.tail = FALSE)
  
  names(STATISTIC) <- "Kruskal-Wallis chi-squared"
  names(PARAMETER) <- "df"
  
  RVAL <- list(statistic = STATISTIC, parameter = PARAMETER,
  p.value = PVAL, method = "Kruskal-Wallis rank sum test")
}
```


### a) Pseudocode

-   If x is a list:

    -   Ensure x has \>=2 non-empty elements/groups

    -   Warn about coercion, if x has non-numeric elements

    -   Warn about ignoration of g, if it is set

    -   Flatten x into single vector

    -   Create corresponding group labels g

-   Else:

    -   Ensure length(x) = length(g)

    -   Convert g to factor

    -   Stop if only one group

-   n = #overall observations

-   Ensure n \>= 2

-   r = ranks(x)

-   Compute TIES = table(x), i.e. #appearances of each value

-   For each group

    -   calculate sum of ranks

    -   calculate elements in group

    -   statistic_group_i = sum-of-ranks^2^ / #elements-in-group

-   statistic = 12\*sum_across_groups(statistic_group_i) / (n(n+1)) - 3(n+1))

-   statistic = statistic / (1-sum(ties^3^ - ties)/(n^3^ - n)) \# adjust for ties

-   p-val = p-value from chi-squared distribution with degrees of freedom=k-1

-   return named list holding statistic=statistic, degrees of freedom=k-1, p-val=p-val, method="Kruskal-Wallis rank sum test"


### b) Samle Calls

**i) Function call with list x**

```{r}
group1 <- c(5.1, 5.5, 5.3)
group2 <- c(6.2, 6.4, 6.1)
group3 <- c(5.9, 6.0, 6.1)

result_list <- kwtest(list(group1, group2, group3))
result_list
```
**ii) FUnction call with vector x and groups g**

```{r}
x <- c(5.1, 5.5, 5.3, 6.2, 6.4, 6.1, 5.9, 6.0, 6.1)
g <- c(1, 1, 1, 2, 2, 2, 3, 3, 3)

result_vector <- kwtest(x, g)
result_vector
```


### c) Refactored Code (Performance Improvements)

```{r}
kwtest_num <- function(x, g, ...) {
  if (length(x) != length(g))
    stop("'x' and 'g' must have the same length")
  if(!is.numeric(x)) 
    stop("'x' must be numeric")
  g <- factor(g)
  k <- nlevels(g)
  if (k < 2L)
    stop("all observations are in the same group")
  
  n <- length(x)
  if (n < 2L)
    stop("not enough observations")
  
  r <- rank(x)
  TIES <- table(x)
  
  rank_sums = tapply(r, g, sum)
  group_sizes = tapply(r, g, length)
  
  STATISTIC <- sum(rank_sums^2/group_sizes)
  STATISTIC <- ((12 * STATISTIC/(n * (n + 1)) - 3 * (n + 1)))
  
  # Adjust for ties
  STATISTIC <- STATISTIC / (1 - sum(TIES^3 - TIES)/(n^3 - n))
  
  PARAMETER <- k - 1L
  PVAL <- pchisq(STATISTIC, PARAMETER, lower.tail = FALSE)
  
  names(STATISTIC) <- "Kruskal-Wallis chi-squared"
  names(PARAMETER) <- "df"
  
  RVAL <- list(statistic = STATISTIC, parameter = PARAMETER,
  p.value = PVAL, method = "Kruskal-Wallis rank sum test")
  
  return (RVAL)
}
```

```{r}
x <- c(5.1, 5.5, 5.3, 6.2, 6.4, 6.1, 5.9, 6.0, 6.1)
g <- c(1, 1, 1, 2, 2, 2, 3, 3, 3)

kwtest_num(x, g)
```

### d) Matrix version for original kwtest function

```{r}
set.seed(1234)
m <- 1000 # number of repetitions
n <- 50 # number of individuals
X <- matrix(rt(m * n, df = 10), nrow = m)
grp <- rep(1:3, c(20, 20, 10))
```

```{r}
repeated_kwtest <- function(X, g) {
  res <- apply(X, 1, function(x) {
    stat <- stats:::kruskal.test.default(x, g)
    return(stat$statistic)
  }
  )
  return(res)
}
```


### e) Matrix version for optimized kwtest function

```{r}
repeated_kwtest_opt <- function(X, g) {
  res <- apply(X, 1, function(x) {
    stat <- kwtest_opt(x, g)
    return(stat$statistic)
  }
  )
  return(res)
}
```

### f) Comparison via benchmark

```{r}
library(microbenchmark)

# Benchmark for original kwtest function
runtime_original <- microbenchmark(
  repeated_kwtest(X, grp),
  times = 5
)
runtime_opt <- microbenchmark(
  repeated_kwtest_opt(X, grp),
  times = 5
)

print(runtime_original)
print(runtime_opt)

# Plot the results
autoplot(benchmark_original) + ggtitle("Benchmark for original kwtest function")
autoplot(benchmark_optimized) + ggtitle("Benchmark for optimized kwtest function")
```


### g) Vectorized version of kwtest function

```{r}
kwtest_vec <- function(x, g, ...) {
  if (length(x) != length(g))
    stop("'x' and 'g' must have the same length")
  if(!is.numeric(x)) 
    stop("'x' must be numeric")
  g <- factor(g)
  k <- nlevels(g)
  if (k < 2L)
    stop("all observations are in the same group")
  
  n <- length(x)
  if (n < 2L)
    stop("not enough observations")
  
  r <- rank(x)
  TIES <- table(x)
  
  rank_sums = rowsum(r, g)
  group_sizes = table(g)
  
  STATISTIC <- sum(rank_sums[,1]^2/group_sizes)
  STATISTIC <- ((12 * STATISTIC/(n * (n + 1)) - 3 * (n + 1))/(1 -
  sum(TIES^3 - TIES)/(n^3 - n)))
  
  PARAMETER <- k - 1L
  PVAL <- pchisq(STATISTIC, PARAMETER, lower.tail = FALSE)
  
  names(STATISTIC) <- "Kruskal-Wallis chi-squared"
  names(PARAMETER) <- "df"
  
  RVAL <- list(statistic = STATISTIC, parameter = PARAMETER,
  p.value = PVAL, method = "Kruskal-Wallis rank sum test")
  
  return (RVAL)
}
```


Comparison:

```{r}  
runtime_vec <- microbenchmark(
  repeated_kwtest_vec(X, grp),
  times = 5
)

print(runtime_vec)
```

